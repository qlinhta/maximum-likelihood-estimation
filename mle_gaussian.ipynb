{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation for Gaussian Random Variables\n",
    "\n",
    "##### ID CIMA-ML : 324982348MLS\n",
    "##### SD : Quyen Linh TA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial demonstrates how to find the maximum likelihood estimates of the mean $\\mu$ and the variance $\\sigma^2$ both analytically and using gradient descent for a Gaussian random variable $X$. The probability density function of $X$ is:\n",
    "\n",
    "$$f_X(x;\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left({-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\right)$$\n",
    "\n",
    "Suppose that you observe 500 independent and identically distributed (i.i.d) samples of $X$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 500\n",
      "Variance: 9.661876678466797\n",
      "Mean: 1.8334250450134277\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "mu = 2\n",
    "sigma = 3\n",
    "num_samples = 500\n",
    "\n",
    "X = torch.randn((num_samples,),requires_grad = False)\n",
    "X = sigma*X + mu\n",
    "\n",
    "print('Number of samples: {}\\nVariance: {}\\nMean: {}'.format(X.shape[0],*torch.var_mean(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The independence assumption is valid if observing one of the samples does not give you information about the other samples, while the identical distribution assumption makes sense if these observations originated from the same underlying random experiment. Therefore, the dataset $\\mathcal{D}$ is:\n",
    "\n",
    "$$\\mathcal{D} = \\{X = x_1,X = x_2,...,X = x_{500}\\}$$\n",
    "\n",
    "The likelihood function is therefore:\n",
    "\n",
    "$$ p(\\mathcal{D};\\mu,\\sigma^2) = \\prod_{i=1}^{500}f_X(x_i;\\mu_i,\\sigma_{i}^2) = \\prod_{i=1}^{500} \\frac{1}{\\sqrt{2\\pi\\sigma_{i}^2}} \\exp\\left({-\\frac{(x_i-\\mu_i)^2}{2\\sigma_{i}^2}}\\right) $$\n",
    "\n",
    "Since the samples are identically distributed:\n",
    "\n",
    "$$\n",
    "\\mu_1 = \\mu_2 = ... = \\mu_{500} = \\mu \\\\\n",
    "\\sigma_{1}^2 = \\sigma_{2}^2 = ... = \\sigma_{500}^2 = \\sigma^2\n",
    "$$\n",
    "\n",
    "Then the log-likelihood function is:\n",
    "\n",
    "\\begin{align}\n",
    "\\ln(p(\\mathcal{D};\\mu,\\sigma^2)) &= \\sum_{i=1}^{500} \\ln\\left(\\frac{1}{\\sqrt{2\\pi\\sigma_{i}^2}}\\right) + \\ln\\left(\\exp\\left({-\\frac{(x_i-\\mu_i)^2}{2\\sigma_{i}^2}}\\right)\\right) \\\\\n",
    "&= -\\left(\\sum_{i=1}^{500} \\ln\\left(\\sqrt{2\\pi\\sigma_{i}^2}\\right) + \\sum_{i=1}^{500} \\frac{(x_i-\\mu_i)^2}{2\\sigma_{i}^2}\\right) \\\\\n",
    "&= -\\left(500 \\ln\\left(\\sqrt{2\\pi\\sigma^2}\\right) + \\frac{1}{2\\sigma^2}\\sum_{i=1}^{500} (x_i-\\mu)^2 \\right)\n",
    "\\end{align}\n",
    "\n",
    "The negative of the log-likelihood function can be implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1605.3162)\n"
     ]
    }
   ],
   "source": [
    "# Negative log-likelihood of X for a given mu and sigma\n",
    "\n",
    "from math import pi\n",
    "\n",
    "def normal_NLL(X,theta): # theta[0] = mu, theta[1] = sigma^2\n",
    "    first_term = X.shape[0]*torch.log(torch.sqrt(2*pi*theta[1]))\n",
    "    second_term = torch.div(1.,2*theta[1])*torch.sum(torch.pow(X-theta[0],2))\n",
    "    return (first_term + second_term)\n",
    "\n",
    "theta = torch.rand((2,))*10\n",
    "NLL = normal_NLL(X,theta)\n",
    "print(NLL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the maximum likelihood estimates of $\\mu$ and $\\sigma^2$ are:\n",
    "\n",
    "\\begin{equation}\n",
    "\\DeclareMathOperator*{\\argmin}{\\arg\\!\\min}\n",
    "\\hat{\\mu} = \\argmin_{\\mu}{\\left(-\\ln\\left(p\\left(\\mathcal{D};\\mu,\\sigma^2\\right)\\right)\\right)} =  \\argmin_{\\mu}\\sum_{i=1}^{500} (x_i-\\mu)^2\\\\\n",
    "\\hat{\\sigma}^2 = \\argmin_{\\sigma^2}{\\left(-\\ln\\left(p\\left(\\mathcal{D};\\mu,\\sigma^2\\right)\\right)\\right)} = \\argmin_{\\sigma^2} \\left(500\\ln\\left(\\sqrt{2\\pi\\sigma^2}\\right) + \\frac{1}{2\\sigma^2}\\sum_{i=1}^{500}(x_i-\\mu)^2\\right)\n",
    "\\end{equation}\n",
    "\n",
    "$\\hat{\\mu}$ and $\\hat{\\sigma}^2$ can be computed analytically. To compute $\\hat{\\mu}$:\n",
    "\n",
    "\\begin{align}\n",
    "- \\frac{\\partial\\ln\\left(p\\left(\\mathcal{D};\\mu,\\sigma^2\\right)\\right)}{\\partial\\mu} &= -2 \\sum_{i=1}^{500} (x_i-\\mu) \\\\\n",
    "&= -2 \\left(\\sum_{i=1}^{500} x_i - \\sum_{i=1}^{500} \\mu\\right) \\\\\n",
    "&= -2 \\left(\\sum_{i=1}^{500} x_i - 500\\mu\\right) = 0\n",
    "\\end{align}\n",
    "\n",
    "Solving for $\\mu$:\n",
    "\n",
    "$$\\hat{\\mu} = \\frac{\\sum_{i=1}^{500} x_i}{500}$$\n",
    "\n",
    "Which is just the sample mean. Similarly, to compute the variance $\\hat{\\sigma}^2$:\n",
    "\n",
    "$$\n",
    "-\\frac{\\partial\\ln\\left(p\\left(\\mathcal{D};\\mu,\\sigma^2\\right)\\right)}{\\partial\\sigma^2} = \\frac{500}{2\\sigma^2} - \\frac{1}{2\\sigma^4} \\sum_{i=1}^{500} (x_i-\\mu)^2 = 0\n",
    "$$\n",
    "\n",
    "Solving for $\\sigma^2$:\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\frac{1}{500} \\sum_{i=1}^{500} (x_i-\\mu)^2\n",
    "$$\n",
    "\n",
    "Which is just the sample variance. However, this is a biased estimator of $\\sigma^2$. Instead, an unbiased estimate of $\\sigma^2$ can be computed as follows:\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\frac{1}{500-1} \\sum_{i=1}^{500} (x_i-\\mu)^2\n",
    "$$\n",
    "\n",
    "This modification is called [Bessel's correction](https://en.wikipedia.org/wiki/Bessel%27s_correction). The maximum likelihood estimates of $\\mu$ and $\\sigma^2$ can then be computed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8334)\n"
     ]
    }
   ],
   "source": [
    "# sample mean\n",
    "\n",
    "mu_hat = torch.sum(X)/X.shape[0]\n",
    "\n",
    "print(mu_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.6619)\n"
     ]
    }
   ],
   "source": [
    "# sample variance\n",
    "\n",
    "sigma_squared_hat = torch.sum(torch.pow(X-mu_hat,2))/(X.shape[0]-1)\n",
    "\n",
    "print(sigma_squared_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, the maximum likelihood estimates of $\\mu$ and $\\sigma^2$ can be computed using gradient descent. Gradient descent is an iterative algorithm based on the following general methodology:\n",
    "1. Guess an initial value for the parameter vector $\\mathbf{\\theta} = (\\mu,\\sigma^2)$.\n",
    "2. Compute the gradient of the negative log-likeihood function at these initial values of $\\mu$ and $\\sigma^2$. More precisely, if $\\mathbf{\\theta}_0 = (\\mu_0,\\sigma_0^2)$ is the initial parameter vector, then compute:\n",
    "\n",
    "$$\n",
    "-\\left.\\frac{\\partial\\ln\\left(p\\left(\\mathcal{D};\\mathbf{\\theta}\\right)\\right)}{\\partial\\mathbf{\\theta}} \\right\\rvert_{\\mathbf{\\theta} = \\mathbf{\\theta}_0}\n",
    "$$\n",
    "\n",
    "Note that the vector $\\mathbf{\\theta} = (\\mu,\\sigma^2)$ was used here because the individual partial derivatives of the negative log-likelihood function with respect to $\\mu$ and $\\sigma^2$ can only be used to perform minimization exclusively with respect to $\\mu$ or $\\sigma^2$ but not both at the same time.\n",
    "\n",
    "3. Use some function:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\theta}_1 = g\\left(\\mathbf{\\theta}_0,-\\left.\\frac{\\partial\\ln\\left(p\\left(\\mathcal{D};\\mathbf{\\theta}\\right)\\right)}{\\partial\\mathbf{\\theta}} \\right\\rvert_{\\mathbf{\\theta} = \\mathbf{\\theta}_0}\\right)\n",
    "$$\n",
    "\n",
    "To choose a new parameter vector $\\mathbf{\\theta}_1$ that minimizes the negative log-likelihood function.\n",
    "\n",
    "4. Repeat steps 1,2, and 3 until convergence.\n",
    "\n",
    "The following explanation for how to choose $g(\\cdot)$ is adapted from [here](https://eli.thegreenplace.net/2016/understanding-gradient-descent/). Consider the simple function $f(x) = x^2$. The value of $f(x)$ decreases when x is negative and increasing, while its value increases when x is positive and increasing. Given an infinitesimally small change $dx$, such that:\n",
    "\n",
    "\\begin{align}\n",
    "f(x)+df(x) &= (x+dx)^2 \\\\\n",
    "f(x)+df(x) &= x^2 + 2x dx + {dx}^2 \\\\\n",
    "x^2 + df(x) &= x^2 + 2x dx \\\\\n",
    "df(x) &= 2x dx \\\\\n",
    "\\frac{df(x)}{dx} &= 2x\n",
    "\\end{align}\n",
    "\n",
    "Suppose that you do not know where the minimum of $f(x)$ is. Let us guess first that the minimum is at $x = -2$. The derivative of $f(x)$ at $x = -2$ is $-4$, which means that an infinitely small increase in $x$, denoted as $dx$, will result in a decrease in $f(x)$ by a factor of 4. Thus, it makes sense to increase $x$ in order to reach the minimum of $f(x)$. In fact, for any convex function $f(x)$, the negative of its derivative $\\frac{df(x)}{dx}$ will always \"point\" towards its minimum point. So, it makes sense to make a new guess $x_{new}$ such that:\n",
    "\n",
    "$$\n",
    "x_{new} = x_{old} - \\eta\\left.\\frac{df(x)}{dx}\\right\\rvert_{x = x_{old}}\n",
    "$$\n",
    "\n",
    "Where $\\eta$ is a small constant called the _learning rate_ that controls the magnitude of $\\frac{df(x)}{dx}$ and thus controls the magnitude of each change of $x$.\n",
    "\n",
    "For multivariate functions, such as $f(\\mathbf{\\theta})$ where $\\mathbf{\\theta} = (x,y,z)$, it can be shown that its gradient $\\nabla_{\\mathbf{\\theta}}f(\\mathbf{\\theta})$ points in the direction of steepest ascent, and so its negative points in the direction of steepest descent. Therefore, for multivariate functions, the update rule becomes:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\theta}_{new} = \\mathbf{\\theta}_{old} - \\eta \\left.\\nabla_{\\mathbf{\\theta}}f(\\mathbf{\\theta})\\right\\rvert_{\\mathbf{\\theta} = \\mathbf{\\theta}_{old}}\n",
    "$$\n",
    "\n",
    "Therefore, in the case of $\\mu$ and $\\sigma^2$, the required gradient is:\n",
    "\n",
    "$$\n",
    "-\\left.\\frac{\\partial\\ln\\left(p\\left(\\mathcal{D};\\mathbf{\\theta}\\right)\\right)}{\\partial\\mathbf{\\theta}} \\right\\rvert_{\\mathbf{\\theta} = \\mathbf{\\theta}_{old}}\n",
    "$$\n",
    "\n",
    "Where $\\mathbf{\\theta}_{old} = (\\mu_{old},\\sigma^2_{old})$.\n",
    "\n",
    "Gradient descent can be implemented as follows. First, an initial theta is chosen at random, and its gradient is tracked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8.9902, 1.0573], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "theta_old = torch.rand((2,),requires_grad = False)\n",
    "theta_old = theta_old*10 # sample uniformly from range [0,10]\n",
    "theta_old.requires_grad = True # theta_old is now a leaf tensor\n",
    "\n",
    "print(theta_old)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a learning rate is chosen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the negative log-likelihood of $X$ with respect to the initial theta is computed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14864.4482, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "NLL = normal_NLL(X,theta_old)\n",
    "\n",
    "print(NLL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of the negative log-likelihood function with respect to theta is then computed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLL.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new value of theta is then computed using the gradient descent update rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5.6058, 14.4320], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "theta_new = theta_old - lr*theta_old.grad\n",
    "theta_old.grad.zero_() # zero gradients accumulated in .grad\n",
    "\n",
    "print(theta_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, this process is repeated until convergence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-10\n",
    "\n",
    "while torch.norm(theta_new - theta_old).item() > eps:\n",
    "    theta_old = theta_new.detach().requires_grad_()\n",
    "    NLL = normal_NLL(X,theta_old)\n",
    "    NLL.backward()\n",
    "    theta_new = theta_old - lr*theta_old.grad\n",
    "    theta_old.grad.zero_() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final estimates of $\\mu$ and $\\sigma^2$ using gradient descent are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum likelihood estimate of mean: 1.8334261178970337\n",
      "Maximum likelihood estimate of variance: 9.642729759216309\n"
     ]
    }
   ],
   "source": [
    "print('Maximum likelihood estimate of mean: {}'.format(theta_new[0].item()))\n",
    "print('Maximum likelihood estimate of variance: {}'.format(theta_new[1].item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the sample mean and variance that were analytically computed are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample mean: 1.8334249258041382\n",
      "Sample variance: 9.661876678466797\n"
     ]
    }
   ],
   "source": [
    "print('Sample mean: {}\\nSample variance: {}'.format(mu_hat,sigma_squared_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Data Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  3\n",
      "STD:  0.8\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "#Set Mean and Standard Deviation of the red points\n",
    "red_mean = 3\n",
    "red_std = 0.8\n",
    "\n",
    "\n",
    "# Number of the points to generate from the distribution\n",
    "numberOfRedPoints = 400\n",
    "\n",
    "\n",
    "# Generate red points using the mean and std\n",
    "red = np.random.normal(red_mean, red_std, numberOfRedPoints)\n",
    "\n",
    "\n",
    "print(\"Mean: \", red_mean)\n",
    "print(\"STD: \", red_std)\n",
    "\n",
    "\n",
    "# Plot the red points\n",
    "plt.rcParams['figure.figsize'] = (15, 2)\n",
    "plt.plot(red, np.zeros_like(red), '.', color='r', markersize=10)\n",
    "plt.title('Distribution of Sample Points', fontsize=17)\n",
    "plt.yticks([]); # Doesn't show the y coordinates\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the likelihood of the red and blue distributions by computing respective PDF's\n",
    "red_likelihood = stats.norm(red_mean, red_std).pdf(red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
